# -*- coding: utf-8 -*-
"""Hybrid_Recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18klsMsxZ36_wWJC1d2dXdTgjUYyJfL8k

# Imports & Utils
"""

# Imports
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.nn.functional import cosine_similarity
import torch.nn.functional as F
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.manifold import TSNE
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
import joblib
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

def set_seed(seed: int = 42):
    """Set seed for reproducibility across PyTorch, NumPy, and CUDA."""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(1)

"""# Loading in Data"""

users = pd.read_csv('users.csv')
users.head()

meals = pd.read_csv('meals.csv')
meals.head()

ratings = pd.read_csv('ratings.csv')

"""# EDA

## Ratings
"""

ratings.info()

sns.countplot(data=ratings, x='rating')
plt.title('Distribution of meals ratings')
plt.show()

global_average_rating = ratings['rating'].mean()
print(f'Global mean rating:{round(global_average_rating, 3)}')

mean_rating_per_user = ratings.groupby("user_id")['rating'].mean()
print(f'Mean rating per user:{round((mean_rating_per_user).mean(), 3)}')

mean_rating_per_meal = ratings.groupby('meal_id')['rating'].mean()
print(f'Mean rating per meal:{round((mean_rating_per_meal).mean(), 3)}')

# Bayesian avg for ratings
average_ratings_per_meal = ratings['meal_id'].value_counts().mean()
C = average_ratings_per_meal

meal_stats = ratings.groupby('meal_id')['rating'].agg(['sum', 'count']).reset_index()
meal_stats.rename(columns={'sum': 'sum_ratings', 'count': 'num_ratings'}, inplace=True)

meal_stats['bayesian_rating'] = round((C * global_average_rating + meal_stats['sum_ratings']) / (C + meal_stats['num_ratings']), 2)

meal_stats = meal_stats.merge(meals, left_on='meal_id', right_index=True)
meal_stats.sort_values(by='bayesian_rating', ascending=False)[['bayesian_rating', 'title']].head(10)

meal_ratings = ratings.merge(meals, left_on='meal_id', right_index=True)
meal_ratings['title'].value_counts()[:10]

"""## Users"""

users.info()

users.drop(columns=["id"]).describe()

"""## Meals"""

meals.info()

meals.drop(columns=["id"]).describe()

"""# Pre-processing"""

# @title Hyperparameters
hyperparams = {
    "embedding_dim": 32,
    "nhead": 2,
    "ff_dim": 32,
    "n_layers": 2,
    "dropout_prob": 0.4,
    "emb_dropout": 0.2,
    "epochs": 30,
    "batch": 512,
    "LR": 3e-2,
    "weight_decay": {"embedding_wd": 1e-4, "bias_wd": 0.0, "other_wd": 5e-3},
    "scheduler": {
        "factor": 0.7,
        "patience": 2,
        "threshold": 1e-3,
        "min_lr": 1e-7
    }
}

# @title Load raw data

DATA_DIR = "."
users = pd.read_csv(os.path.join(DATA_DIR, "users.csv"))
meals = pd.read_csv(os.path.join(DATA_DIR, "meals.csv"))
ratings = pd.read_csv(os.path.join(DATA_DIR, "ratings.csv"))

# Filter ratings to include only users present in the users DataFrame
ratings = ratings[ratings['user_id'].isin(users['id'])]

# Filter ratings to include only meals present in the meals DataFrame
ratings = ratings[ratings['meal_id'].isin(meals['id'])]

# Convert user_id and meal_id columns to integer type
ratings['user_id'] = ratings['user_id'].astype(int)
ratings['meal_id'] = ratings['meal_id'].astype(int)

# Train / validation / test split on the ratings

train_ratings, temp_ratings = train_test_split(
    ratings, test_size=0.3, random_state=42) # 70 % train

val_ratings, test_ratings = train_test_split(
    temp_ratings, test_size=0.5, random_state=42) # 15 % val, 15 % test

train_users = users[users["id"].isin(train_ratings["user_id"])]
train_meals = meals[meals["id"].isin(train_ratings["meal_id"])]

# Data pipelines

#  – user side
USER_CAT = ["Gender", "Activity Level"]
USER_NUM = ["Ages", "Height", "Weight", "Daily Calorie Target",
            "Protein", "Sugar", "Sodium", "Calories",
            "Carbohydrates", "Fiber", "Fat"]
USER_READY = ['Acne','Diabetes','Heart Disease','Hypertension','Kidney Disease',
              'Weight Gain','Weight Loss']

user_preproc = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), USER_CAT),
    ("num", StandardScaler(), USER_NUM),
    ("ready", "passthrough", USER_READY)
])

# Fit on train users
user_preproc.fit(train_users.set_index("id")[USER_CAT + USER_NUM + USER_READY])

# Transform all users with fitted preproc
user_features = user_preproc.transform(
    users.set_index("id")[USER_CAT + USER_NUM + USER_READY]
).astype("float32")


#  – meal side
MEAL_CAT = ["allergen_name"]
MEAL_NUM = ["price_cents", "sugar", "calories", "fat",
            "protein", "fiber", "carbohydrates", "sodium"]

meal_preproc = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), MEAL_CAT),
    ("num", StandardScaler(), MEAL_NUM)
])

# Fit on train meals
meal_preproc.fit(train_meals.set_index("id")[MEAL_CAT + MEAL_NUM])

# Transform all meals with fitted preproc
meal_features = meal_preproc.transform(
    meals.set_index("id")[MEAL_CAT + MEAL_NUM]
).astype("float32")

# Build look-up tables for quick tensor retrieval
uid_to_idx = {orig_id: idx for idx, orig_id in enumerate(users["id"])}
mid_to_idx = {orig_id: idx for idx, orig_id in enumerate(meals["id"])}

# reverse map
idx_to_mid = {idx: mid for mid, idx in mid_to_idx.items()}
idx_to_uid = {idx: uid for uid, idx in uid_to_idx.items()}

# @title Description encoding
from sentence_transformers import SentenceTransformer

descriptions = meals["description"].astype(str).tolist()

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
desc_vecs = model.encode(descriptions, batch_size=64, show_progress_bar=True)

save_path = "meal_desc_384.npy"
np.save(save_path, desc_vecs.astype(np.float32))
print("Saved", desc_vecs.shape, "vectors to", save_path)

# @title MealRating Dataset
class MealRatingDataset(Dataset):
    def __init__(self, df):
        self.df = df.reset_index(drop=True)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        u_idx = uid_to_idx[row["user_id"]]
        m_idx = mid_to_idx[row["meal_id"]]
        label = float(row["rating"])

        meal_feat = torch.cat([
            torch.from_numpy(meal_features[m_idx]),
            torch.from_numpy(desc_vecs[m_idx])
        ], dim=0)

        return {
            "user_id" : torch.tensor(u_idx, dtype=torch.long),
            "meal_id" : torch.tensor(m_idx, dtype=torch.long),
            "user_feat" : torch.from_numpy(user_features[u_idx]),
            "meal_feat" : meal_feat,
            "label" : torch.tensor(label, dtype=torch.float32)
        }

train_ds = MealRatingDataset(train_ratings)
val_ds = MealRatingDataset(val_ratings)
test_ds = MealRatingDataset(test_ratings)

BATCH = hyperparams["batch"]
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)
val_loader = DataLoader(val_ds,   batch_size=BATCH)
test_loader = DataLoader(test_ds,  batch_size=BATCH)

"""# Hybrid Model"""

# @title Transformer-based Hybrid Recommender
class HybridRecommender(nn.Module):
    def __init__(self,
                 n_users,
                 n_meals,
                 user_feat_dim,
                 meal_feat_dim,
                 hyperparams):
        super().__init__()
        self.hyperparams = hyperparams
        d = hyperparams["embedding_dim"]

        # embeddings & biases
        self.user_emb = nn.Embedding(n_users, d)
        self.meal_emb = nn.Embedding(n_meals, d)
        self.user_bias  = nn.Embedding(n_users, 1)
        self.meal_bias  = nn.Embedding(n_meals, 1)

        self.user_emb_dropout = nn.Dropout(p=hyperparams["emb_dropout"])
        self.meal_emb_dropout = nn.Dropout(p=hyperparams["emb_dropout"])

        self.user_feat_proj = nn.Linear(user_feat_dim, d)
        self.meal_feat_proj = nn.Linear(meal_feat_dim, d)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d,
            nhead=hyperparams["nhead"],
            dim_feedforward=hyperparams["ff_dim"],
            dropout=hyperparams["dropout_prob"],
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer,
                                                 num_layers=hyperparams["n_layers"])

        # final projection to score
        self.out_proj = nn.Linear(d, 1)

    def forward(self, user_ids, meal_ids, user_feats, meal_feats):
        u = self.user_emb_dropout(self.user_emb(user_ids))
        m = self.meal_emb_dropout(self.meal_emb(meal_ids))

        # Project the dense feature vectors to the same dimension d
        u_feat = self.user_feat_proj(user_feats)
        m_feat = self.meal_feat_proj(meal_feats)

        # Stack into a 4-token sequence
        seq = torch.stack([u, m, u_feat, m_feat], dim=1)

        # Transformer forward
        mask = None  # every token to attend to every other token
        x = self.transformer(seq, src_key_padding_mask=mask)

        # Global mean-pool
        pooled = x.mean(dim=1)

        raw_output = self.out_proj(pooled).squeeze(1)

        # Add biases
        u_bias = self.user_bias(user_ids).squeeze(1)
        m_bias = self.meal_bias(meal_ids).squeeze(1)
        return raw_output + u_bias + m_bias

# Initialize model
n_users = len(uid_to_idx)
n_meals = len(mid_to_idx)

model = HybridRecommender(
    n_users=len(uid_to_idx),
    n_meals=len(mid_to_idx),
    user_feat_dim=user_features.shape[1],
    meal_feat_dim=meal_features.shape[1] + desc_vecs.shape[1],
    hyperparams=hyperparams
).to(device)

# Train setup
LR = hyperparams["LR"]
embedding_wd = hyperparams["weight_decay"]["embedding_wd"]
bias_wd = hyperparams["weight_decay"]["bias_wd"]
other_wd = hyperparams["weight_decay"]["other_wd"]

# Separate param groups for different weight decays
embedding_params, bias_params, other_params = [], [], []
for name, param in model.named_parameters():
    if "emb" in name: embedding_params.append(param)
    elif "bias" in name: bias_params.append(param)
    else: other_params.append(param)

optimizer = torch.optim.Adam([
    {"params": embedding_params, "weight_decay": embedding_wd},
    {"params": bias_params, "weight_decay": bias_wd},
    {"params": other_params, "weight_decay": other_wd}
], lr=LR)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode="min",
    factor=hyperparams["scheduler"]["factor"],
    patience=hyperparams["scheduler"]["patience"],
    threshold=hyperparams["scheduler"]["threshold"],
    min_lr=hyperparams["scheduler"]["min_lr"]
)

loss_fn = nn.MSELoss()
mae_fn = nn.L1Loss()

def run_epoch(loader, training=False):
    if training: model.train()
    else: model.eval()
    total_loss, total_mae, n = 0.0, 0.0, 0
    for batch in loader:
        user_ids = batch["user_id"].to(device)
        meal_ids = batch["meal_id"].to(device)
        user_feats = batch["user_feat"].to(device)
        meal_feats = batch["meal_feat"].to(device)
        labels = batch["label"].to(device)

        if training: optimizer.zero_grad()
        preds = model(user_ids, meal_ids, user_feats, meal_feats)
        loss = loss_fn(preds, labels)
        mae = mae_fn(preds, labels)
        if training:
            loss.backward()
            optimizer.step()
        total_loss += loss.item() * len(labels)
        total_mae  += mae.item() * len(labels)
        n += len(labels)
    rmse = np.sqrt(total_loss / n)
    mae = total_mae / n
    return rmse, mae

# @title Train
train_rmse_history, val_rmse_history = [], []
train_mae_history,  val_mae_history  = [], []
EPOCHS = hyperparams["epochs"]

for epoch in range(1, EPOCHS+1):
    val_rmse, val_mae = run_epoch(val_loader, training=False)
    train_rmse, train_mae = run_epoch(train_loader, training=True)

    scheduler.step(val_rmse)

    train_rmse_history.append(train_rmse)
    val_rmse_history.append(val_rmse)
    train_mae_history.append(train_mae)
    val_mae_history.append(val_mae)

    if epoch == 1 or epoch % 5 == 0:
        print(f"Epoch {epoch:02d} | "
              f"train RMSE {train_rmse:.4f} | val RMSE {val_rmse:.4f} | "
              f"train MAE {train_mae:.4f} | val MAE {val_mae:.4f}")

# @title Plot curves
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(range(EPOCHS), train_rmse_history, label='Train RMSE')
plt.plot(range(EPOCHS), val_rmse_history, label='Val RMSE')
plt.xlabel('Epoch'); plt.ylabel('RMSE'); plt.legend(); plt.title('RMSE')
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(range(EPOCHS), train_mae_history, label='Train MAE')
plt.plot(range(EPOCHS), val_mae_history, label='Val MAE')
plt.xlabel('Epoch'); plt.ylabel('MAE'); plt.legend(); plt.title('MAE')
plt.grid(True)

plt.tight_layout()
plt.show()

# @title Plot curves dropping the first epoch
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(range(EPOCHS-1), train_rmse_history[1:], label='Train RMSE')
plt.plot(range(EPOCHS-1), val_rmse_history[1:], label='Val RMSE')
plt.xlabel('Epoch'); plt.ylabel('RMSE'); plt.legend(); plt.title('RMSE')
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(range(EPOCHS-1), train_mae_history[1:], label='Train MAE')
plt.plot(range(EPOCHS-1), val_mae_history[1:], label='Val MAE')
plt.xlabel('Epoch'); plt.ylabel('MAE'); plt.legend(); plt.title('MAE')
plt.grid(True)

plt.tight_layout()
plt.show()

test_rmse, test_mae = run_epoch(test_loader, training=False)
print(f"Test RMSE: {test_rmse:.4f} | "
      f"Test MAE: {test_mae:.4f}")

# Inference helper – top-N meals for a given user
model.eval()

@torch.inference_mode()
def recommend(user_id: int, k: int = 10):
    model.eval()
    with torch.inference_mode():
        device = next(model.parameters()).device

        # user and candidate meals
        uid = uid_to_idx[user_id]
        rated = set(ratings.loc[ratings["user_id"] == user_id, "meal_id"])
        cand_ids = meals.loc[~meals["id"].isin(rated), "id"].values
        if cand_ids.size == 0:
            return pd.DataFrame(columns=["id", "title", "score"])

        # Create a temporary DataFrame for candidate meals for this user
        cand_ratings_df = pd.DataFrame({
            'user_id': user_id,
            'meal_id': cand_ids,
            'rating': 0 # Dummy rating, not used in prediction
        })

        # Create a Dataset and DataLoader for the candidate meals
        cand_ds = MealRatingDataset(cand_ratings_df)
        cand_loader = DataLoader(cand_ds, batch_size=BATCH)

        all_scores = []
        # Use the DataLoader to get batches of data
        for batch in cand_loader:
            u_ids_batch = batch["user_id"].to(device)
            m_ids_batch = batch["meal_id"].to(device)
            u_feats_batch = batch["user_feat"].to(device)
            m_feats_batch = batch["meal_feat"].to(device)

            # forward pass
            scores_batch = model(u_ids_batch, m_ids_batch, u_feats_batch, m_feats_batch).cpu().numpy()
            all_scores.extend(scores_batch)

        scores = np.array(all_scores)


        # build result
        top_k = np.argpartition(-scores, k)[:k]
        rec_df = (meals[meals["id"].isin(cand_ids[top_k])]
                [["id", "title"]]
                .assign(score=scores[top_k])
                .sort_values("score", ascending=False))
    return rec_df

# quick demo
recommend(user_id=10, k=5)

# Save the model's state dictionary
torch.save(model.state_dict(), "hybrid_recommender.pt")

# Save artifacts needed for inference
# Include the preprocessors, and the user and meal index maps
joblib.dump({"user_preprocessor": user_preproc,
             "meal_preprocessor": meal_preproc,
             "uid_to_idx": uid_to_idx,
             "mid_to_idx": mid_to_idx
            }, "artifacts_hybrid_recommender.pkl")

print("Saved model + artifacts.")

"""# Explore Embeddings"""

for name, param in model.named_parameters():
  if param.requires_grad:
    print(f"Parameter name: {name}")
    print(f"Parameter shape: {param.shape}")
    print("-" * 20)

embs = {}

# @title Extract parameters from the model
for name, param in model.named_parameters():
  if param.requires_grad:
    embs[name[:-len('.weight')]] = param.data

meal_emb = embs['meal_emb']
users_emb = embs['user_emb']

# @title Top-k meals according to bias
meal_bias = embs['meal_bias'].squeeze()
idxs = meal_bias.argsort(descending=True)[:10]

# Get titles using original ids
original_meal_ids = [idx_to_mid[idx.item()] for idx in idxs]
most_liked_meals = meals[meals['id'].isin(original_meal_ids)]['title'].tolist()

for i, meal in enumerate(most_liked_meals):
  print(f'{i+1}. {meal}')

# @title Top-k similar meals to target meal
target_idx = 1 # index of meal to inspect
emb_target = meal_emb[target_idx].unsqueeze(0) # (1, D)
cos_sim = torch.nn.functional.cosine_similarity(
    emb_target, meal_emb, dim=1
)

top_k = 10
top_indices = cos_sim.argsort(descending=True)[:top_k+1]

top_meal_ids = [idx_to_mid[i.item()] for i in top_indices]
top_titles   = (
    meals[meals["id"].isin(top_meal_ids)]
         .set_index("id")
         .loc[top_meal_ids]["title"]
         .tolist()
)

original_title = meals[meals["id"] == idx_to_mid[target_idx]]["title"].iloc[0]
print(f"Meals similar to '{original_title}':")
for rank, title in enumerate(top_titles[1:], 1):
    print(f"{rank}. {title}")

"""#### Plotting Embeddings"""

# @title Plot meal embeddings
# Convert meal_emb tensor to numpy array
meal_embeddings_np = meal_emb.cpu().numpy()

# Apply t-SNE to reduce dimensions to 2
tsne = TSNE(n_components=2, random_state=0, perplexity=10)
meal_embeddings_2d = tsne.fit_transform(meal_embeddings_np)

# Create a DataFrame for plotting
meal_embeddings_df = pd.DataFrame(meal_embeddings_2d, columns=['tsne_dim1', 'tsne_dim2'])

# Plotting
plt.figure(figsize=(10, 8))
sns.scatterplot(data=meal_embeddings_df, x='tsne_dim1', y='tsne_dim2')
plt.title('2D t-SNE visualization of Meal Embeddings')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid(True)
plt.show()

# @title Plot user embeddings
# Convert user_factors tensor to numpy array
user_embeddings_np = users_emb.cpu().numpy()

# Apply t-SNE to reduce dimensions to 2
tsne_users = TSNE(n_components=2, random_state=42, perplexity=10)
user_embeddings_2d = tsne_users.fit_transform(user_embeddings_np)

# Create a DataFrame for plotting
user_embeddings_df = pd.DataFrame(user_embeddings_2d, columns=['tsne_dim1', 'tsne_dim2'])

# Plotting
plt.figure(figsize=(10, 8))
sns.scatterplot(data=user_embeddings_df, x='tsne_dim1', y='tsne_dim2')
plt.title('2D t-SNE visualization of User Embeddings')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid(True)
plt.show()

"""# Retrieval Model"""

interaction_df = pd.read_csv("interactions.csv")
interaction_df.info()

# Split the data into train and validation sets
train_interaction_df, val_interaction_df = train_test_split(
    interaction_df, test_size=0.2, random_state=42)

# @title Interaction dataset
class InteractionDataset(Dataset):
    def __init__(self, user_ids, meal_ids, labels):
        self.user_ids = user_ids
        self.meal_ids = meal_ids
        self.labels = labels

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        u = self.user_ids[idx]
        m = self.meal_ids[idx]
        y = self.labels[idx]
        return torch.tensor(u, dtype=torch.long), \
               torch.tensor(m, dtype=torch.long), \
               torch.tensor(y, dtype=torch.float)


def collate_fn(batch):
    """Stack tuples into tensors."""
    user, meal, label = zip(*batch)
    return torch.stack(user), torch.stack(meal), torch.stack(label)

train_ds = InteractionDataset(train_interaction_df["user_id"].values,
                            train_interaction_df["meal_id"].values,
                            train_interaction_df["interaction"].values)
val_ds   = InteractionDataset(val_interaction_df["user_id"].values,
                            val_interaction_df["meal_id"].values,
                            val_interaction_df["interaction"].values)

BATCH = 512

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,
                            collate_fn=collate_fn)
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,
                            collate_fn=collate_fn)

# @title RetrievalNet
class RetrievalNet(nn.Module):
    """
    Two-tower retrieval model that re-uses the embeddings
    already trained by HybridRecommender.
    """
    def __init__(self, base: HybridRecommender):
        super().__init__()
        self.user_emb = base.user_emb
        self.meal_emb = base.meal_emb
        # small MLP on top of each tower
        d = base.user_emb.embedding_dim
        self.user_tower  = nn.Sequential(
            nn.Linear(d, d), nn.ReLU(), nn.Linear(d, d)
        )
        self.meal_tower  = nn.Sequential(
            nn.Linear(d, d), nn.ReLU(), nn.Linear(d, d)
        )
        # final dot-product + optional temperature
        # high temperature → scores are small and evenly spread
        # low temperature → scores are large and the model becomes more confident in its top choices
        self.temperature = nn.Parameter(torch.tensor(1.0))

    def forward(self, user_ids, meal_ids):
        u = self.user_tower(self.user_emb(user_ids))
        m = self.meal_tower(self.meal_emb(meal_ids))
        return F.cosine_similarity(u, m, dim=-1) * self.temperature

# Train setup
def train_one_epoch(model, loader, optim, device):
    model.train()
    total_loss = 0.0
    labels, preds = [], []
    for user, meal, label in loader:
        user, meal, label = user.to(device), meal.to(device), label.to(device)
        optim.zero_grad()
        logits = model(user, meal)
        loss = F.binary_cross_entropy_with_logits(logits, label)
        loss.backward()
        optim.step()
        total_loss += loss.item() * user.size(0)

        labels.append(label.cpu())
        preds.append(torch.sigmoid(logits).cpu())

    labels = torch.cat(labels)
    preds = torch.cat(preds)
    auc = roc_auc_score(labels.detach().numpy(), preds.detach().numpy())

    return total_loss / len(loader.dataset), auc


@torch.no_grad()
def evaluate(model, loader, device):
    model.eval()
    labels, preds = [], []
    total_loss = 0.0
    for user, meal, label in loader:
        user, meal, label = user.to(device), meal.to(device), label.to(device)
        logits = model(user, meal)
        loss = F.binary_cross_entropy_with_logits(logits, label)
        total_loss += loss.item() * user.size(0)
        labels.append(label.cpu())
        preds.append(torch.sigmoid(logits).cpu())
    labels = torch.cat(labels)
    preds = torch.cat(preds)

    # Calculate metrics
    auc = roc_auc_score(labels.detach().numpy(), preds.detach().numpy())
    # Convert probabilities to binary predictions (using 0.5 as threshold)
    binary_preds = (preds.detach().numpy() > 0.5).astype(int)
    accuracy = accuracy_score(labels.detach().numpy(), binary_preds)
    f1 = f1_score(labels.detach().numpy(), binary_preds)
    avg_loss = total_loss / len(loader.dataset)

    return avg_loss, auc, accuracy, f1

def recommend_retrieval(user_id: int, k: int = 10, device="cpu"):
    with torch.no_grad():
        retrieval_model.eval()
        if user_id not in uid_to_idx:
            print(f"User {user_id} not found in the dataset.")
            return pd.DataFrame(columns=["id", "title", "score"])

        user_idx = uid_to_idx[user_id]
        user_tensor = torch.tensor([user_idx], device=device)

        # Get all meal indices
        all_meals_tensor = torch.arange(len(mid_to_idx), device=device)

        # Get scores for all meals for the given user
        scores = torch.sigmoid(retrieval_model(user_tensor, all_meals_tensor))  # (N_MEALS,)

        # Get top-k meal indices based on scores
        topk = torch.topk(scores, k=k)
        top_meal_indices = topk.indices.tolist()
        top_scores = topk.values.tolist()

        # Map indices back to original meal IDs
        top_meal_ids = [idx_to_mid[idx] for idx in top_meal_indices]

        # Create a DataFrame with recommended meal titles and scores
        recommended_meals = meals[meals["id"].isin(top_meal_ids)].set_index("id").loc[top_meal_ids]
        recommended_meals["score"] = top_scores

        return recommended_meals[["title", "score"]]

# @title Initialize model
EMBED_DIM = 32
EPOCHS = 5
LR = 2e-2

# Reuse the trained HybridRecommender model
model.eval()
retrieval_model = RetrievalNet(model).to(device)
optim = torch.optim.AdamW(retrieval_model.parameters(), lr=LR)

# @title Train
train_losses, train_aucs, val_losses, val_aucs, val_accuracies, val_f1s = [], [], [], [], [], []
for epoch in range(1, EPOCHS + 1):
    val_loss, val_auc, accuracy, f1 = evaluate(retrieval_model, val_loader, device)
    train_loss, train_auc = train_one_epoch(retrieval_model, train_loader, optim, device)
    train_losses.append(train_loss)
    train_aucs.append(train_auc)
    val_losses.append(val_loss)
    val_aucs.append(val_auc)
    val_accuracies.append(accuracy)
    val_f1s.append(f1)
    print(f"Epoch {epoch:02d} | train-loss {train_loss:.4f} | val-loss {val_loss:.4f} | train-AUC {train_auc:.4f} | val-AUC {val_auc:.4f} | val-Accuracy {accuracy:.4f} | val-F1 {f1:.4f}")

# @title Plot curves
plt.figure(figsize=(10, 8))

plt.subplot(2, 2, 1)
plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')
plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.grid(True)
plt.legend()

plt.subplot(2, 2, 2)
plt.plot(range(1, EPOCHS + 1), train_aucs, label='Train AUC')
plt.plot(range(1, EPOCHS + 1), val_aucs, label='Validation AUC')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.title('AUC')
plt.grid(True)
plt.legend()

plt.subplot(2, 2, 3)
plt.plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.grid(True)
plt.legend()

plt.subplot(2, 2, 4)
plt.plot(range(1, EPOCHS + 1), val_f1s, label='Validation F1 Score')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.title('Validation F1 Score')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# quick top-K demo for user idx
retrieval_model.eval()
user_id = 42 # Original user ID
recommended_meals_df = recommend_retrieval(user_id=user_id, k=10, device=device)
print(f"Top-10 recommended meals for user {user_id}:")
display(recommended_meals_df)

# Save model
torch.save(retrieval_model.state_dict(), "retrieval.pt")

# @title Two-stage Hybrid Recommendation Function
@torch.no_grad()
def recommend_hybrid_retrieval(user_id: int, k: int = 10, num_candidates: int = 50, device="cpu"):
    retrieval_model.eval()
    model.eval()

    if user_id not in uid_to_idx:
        print(f"User {user_id} not found in the dataset.")
        return pd.DataFrame(columns=["id", "title", "score"])

    user_idx = uid_to_idx[user_id]
    user_tensor = torch.tensor([user_idx], device=device)

    # Stage 1: Retrieval (get initial candidates)
    all_meals_tensor = torch.arange(len(mid_to_idx), device=device)
    retrieval_scores = torch.sigmoid(retrieval_model(user_tensor, all_meals_tensor))

    # Get top_candidates meal indices from the retrieval model
    top_candidate_indices = torch.topk(retrieval_scores, k=num_candidates).indices.tolist()
    candidate_meal_ids = [idx_to_mid[idx] for idx in top_candidate_indices]

    # Filter out meals the user has already rated
    rated_meal_ids = set(ratings.loc[ratings["user_id"] == user_id, "meal_id"])
    candidate_meal_ids = [meal_id for meal_id in candidate_meal_ids if meal_id not in rated_meal_ids]

    if not candidate_meal_ids:
      print(f"No new candidate meals found for user {user_id}.")
      return pd.DataFrame(columns=["id", "title", "score"])

    # Stage 2: Reranking (use Hybrid model on candidates)

    # Create a temporary DataFrame for candidate meals for this user for the Hybrid model
    cand_ratings_df = pd.DataFrame({
        'user_id': user_id,
        'meal_id': candidate_meal_ids,
        'rating': 0 # Dummy rating, not used in prediction
    })

    # Create a Dataset and DataLoader for the candidate meals
    cand_ds = MealRatingDataset(cand_ratings_df)
    cand_loader = DataLoader(cand_ds, batch_size=hyperparams["batch"]) # Use hybrid model batch size

    all_rerank_scores = []
    # Use the DataLoader to get batches of data
    for batch in cand_loader:
        u_ids_batch = batch["user_id"].to(device)
        m_ids_batch = batch["meal_id"].to(device)
        u_feats_batch = batch["user_feat"].to(device)
        m_feats_batch = batch["meal_feat"].to(device)

        # forward pass through the hybrid model
        scores_batch = model(u_ids_batch, m_ids_batch, u_feats_batch, m_feats_batch).cpu().numpy()
        all_rerank_scores.extend(scores_batch)

    rerank_scores = np.array(all_rerank_scores)

    # Get top-k indices from the reranked scores
    top_k_indices = np.argpartition(-rerank_scores, k)[:k]

    # Get the corresponding meal IDs and scores
    final_recommendation_ids = [candidate_meal_ids[i] for i in top_k_indices]
    final_recommendation_scores = rerank_scores[top_k_indices]

    # Create a DataFrame with recommended meal IDs and scores
    recommended_meals_df = pd.DataFrame({
        'id': final_recommendation_ids,
        'score': final_recommendation_scores
    })

    # Return meal IDs and scores
    return recommended_meals_df.sort_values("score", ascending=False).reset_index(drop=True)

recommend_hybrid_retrieval(1697)